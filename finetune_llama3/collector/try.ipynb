{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install langchain openai tiktoken chromadb pypdf sentence_transformers InstructorEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wwwya\\OneDrive\\Desktop\\finetune_llama3\\.venv\\lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "DB_FAISS_PATH = 'collector/store/db_faiss'\n",
    "def generate_and_save_embeddings(texts):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                        model_kwargs={'device': 'cpu'})\n",
    "\n",
    "    # Generate the embeddings\n",
    "    doc_embeddings = [embeddings.embed_query(text) for text in texts]\n",
    "\n",
    "    # Save the embeddings to a file\n",
    "    np.save('embeddings.npy', doc_embeddings)\n",
    "\n",
    "def load_data():\n",
    "    loader = CSVLoader(file_path=\"data_car.csv\", encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "    data = loader.load()\n",
    "\n",
    "    # Load the embeddings from a file\n",
    "    embeddings = np.load('embeddings.npy')\n",
    "\n",
    "    db = FAISS.from_documents(data, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data.txt',encoding='utf-8') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wwwya\\OneDrive\\Desktop\\New folder\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generate_and_save_embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_embeddings(texts):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                        model_kwargs={'device': 'cpu'})\n",
    "\n",
    "    # Generate the embeddings\n",
    "    doc_embeddings = [embeddings.embed_query(text) for text in texts]\n",
    "\n",
    "    # Save the embeddings to a file\n",
    "    np.save('embeddings.npy', doc_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    loader = CSVLoader(file_path=\"data_car.csv\", encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "    data = loader.load()\n",
    "\n",
    "    # Load the embeddings from a file\n",
    "    embeddings = np.load('embeddings.npy')\n",
    "\n",
    "    # Create a list of Document objects from the data\n",
    "    documents = [Document(page_content=doc.page_content) for doc in data]\n",
    "\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 14\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Create a list of Document objects from the data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m documents \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mdoc\u001b[38;5;241m.\u001b[39mpage_content) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m---> 14\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m db\u001b[38;5;241m.\u001b[39msave_local(DB_FAISS_PATH)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m db\n",
      "File \u001b[1;32mc:\\Users\\wwwya\\OneDrive\\Desktop\\New folder\\.venv\\lib\\site-packages\\langchain_core\\vectorstores.py:550\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    549\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wwwya\\OneDrive\\Desktop\\New folder\\.venv\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:930\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    911\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 930\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    932\u001b[0m         texts,\n\u001b[0;32m    933\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    938\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone\n",
      "  Using cached pinecone-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\wwwya\\onedrive\\desktop\\new folder\\.venv\\lib\\site-packages (from pinecone) (2024.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\wwwya\\onedrive\\desktop\\new folder\\.venv\\lib\\site-packages (from pinecone) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\wwwya\\onedrive\\desktop\\new folder\\.venv\\lib\\site-packages (from pinecone) (4.12.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\wwwya\\onedrive\\desktop\\new folder\\.venv\\lib\\site-packages (from pinecone) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wwwya\\onedrive\\desktop\\new folder\\.venv\\lib\\site-packages (from tqdm>=4.64.1->pinecone) (0.4.6)\n",
      "Using cached pinecone-4.0.0-py3-none-any.whl (214 kB)\n",
      "Installing collected packages: pinecone\n",
      "Successfully installed pinecone-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from pinecone import Pinecone ,ServerlessSpec\n",
    "retrever = SentenceTransformer(\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",device='cpu')\n",
    "pc = Pinecone(api_key=\"33fd2367-2090-4ff2-b22f-cc2503c2e1ee\")\n",
    "index_name = \"eurotechuser\"\n",
    "index = pc.Index(index_name)\n",
    "retrever = SentenceTransformer(\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",device='cpu')\n",
    "pc = Pinecone(api_key=\"33fd2367-2090-4ff2-b22f-cc2503c2e1ee\")\n",
    "def query_pinecone(query,top_k):\n",
    "    xq = retrever.encode([query]).tolist()\n",
    "    xc = index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
    "    return xc\n",
    "def format_query(query,context):\n",
    "    context = [f\"<P> {m['metadata']['paragraph']}\" for m in context]\n",
    "    context = \" \".join(context)\n",
    "    query =  f\"Querstion: {query} context: {context}\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '502',\n",
       "              'metadata': {'heading': '',\n",
       "                           'paragraph': 'If you have any questions about this '\n",
       "                                        \"policy or Eurotech Xchange's privacy \"\n",
       "                                        'practices, please contact Eurotech '\n",
       "                                        'Xchange.'},\n",
       "              'score': 0.614123,\n",
       "              'values': []},\n",
       "             {'id': '484',\n",
       "              'metadata': {'heading': '',\n",
       "                           'paragraph': \"on Eurotech Xchange's website to \"\n",
       "                                        'other personal information you may '\n",
       "                                        'provide to Eurotech Xchange at the'},\n",
       "              'score': 0.587626278,\n",
       "              'values': []},\n",
       "             {'id': '472',\n",
       "              'metadata': {'heading': '',\n",
       "                           'paragraph': 'information about you as described in '\n",
       "                                        'this policy, which Eurotech Xchange '\n",
       "                                        'handles as stated in this policy.'},\n",
       "              'score': 0.574203372,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"i want to know about the policys of eurotech\"\n",
    "result = query_pinecone(query,top_k=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
